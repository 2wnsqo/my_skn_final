"""
ì‹¤ì‹œê°„ ë©´ì ‘ í‰ê°€ ì‹œìŠ¤í…œ - ìµœì¢… í‰ê°€ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ realtime_result.jsonì— ëˆ„ì ëœ ê°œë³„ í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ
ìµœì¢… í†µí•© í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ì—¬ final_evaluation_results.jsonì„ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ì‹¤ì‹œê°„ ê²°ê³¼ë¥¼ ìµœì¢… í‰ê°€ í˜•íƒœë¡œ ë³€í™˜ (MLì ìˆ˜ + LLMí‰ê°€ â†’ í†µí•© ì ìˆ˜)
2. ê°œë³„ ì§ˆë¬¸ë³„ ìµœì¢… ì ìˆ˜, ì˜ë„, í‰ê°€, ê°œì„ ì‚¬í•­ ìƒì„±
3. ì „ì²´ ë©´ì ‘ì— ëŒ€í•œ ì¢…í•© í‰ê°€ ë° ì ìˆ˜ ì‚°ì¶œ

ì‘ì„±ì: AI Assistant  
"""

from openai import OpenAI
import json
import re
import os
import datetime
from num_eval import score_interview_data, load_interview_data, load_encoder, load_model
from text_eval import evaluate_all

client = OpenAI()

# ìµœì¢… í‰ê°€ìš© ëª¨ë¸ ì„¤ì • (ê°œë³„ í‰ê°€ì™€ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)
FINAL_EVAL_MODEL = "gpt-4o"  # ë˜ëŠ” "gpt-4", "gpt-3.5-turbo", "claude-3-sonnet" ë“±
OVERALL_EVAL_MODEL = "gpt-4o"  # ì „ì²´ ì¢…í•© í‰ê°€ìš© ëª¨ë¸ (ë‹¤ì‹œ ë‹¤ë¥¸ ëª¨ë¸ ê°€ëŠ¥)

# === í•¸ìˆ˜ ì •ì˜ ì„¹ì…˜ ===

def build_final_prompt(q, a, ml_score, llm_feedback):
    """
    ê°œë³„ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… í†µí•© í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
    
    Args:
        q (str): ë©´ì ‘ ì§ˆë¬¸
        a (str): ì§€ì›ì ë‹µë³€
        ml_score (float): ML ëª¨ë¸ ì˜ˆì¸¡ ì ìˆ˜
        llm_feedback (str): LLMì—ì„œ ìƒì„±ëœ ìƒì„¸ í‰ê°€ ê²°ê³¼
        
    Returns:
        str: GPT-4oì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
    """
    return fr"""
[ì§ˆë¬¸]: {q}
[ë‹µë³€]: {a}
[ë¨¸ì‹ ëŸ¬ë‹ ì ìˆ˜]: {ml_score:.1f} (ML ëª¨ë¸ ì˜ˆì¸¡ ì ìˆ˜, ì¼ë°˜ì  ë²”ìœ„: 10~50ì )
[LLM í‰ê°€ê²°ê³¼]: {llm_feedback}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ í•­ëª©ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

1. ğŸ“ ì§ˆë¬¸ ì˜ë„: ì´ ì§ˆë¬¸ì´ ë¬´ì—‡ì„ í‰ê°€í•˜ê³ ì í•˜ëŠ”ì§€ë¥¼ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
2. ğŸ’¬ í‰ê°€: ë‹µë³€ì˜ ê°•ì ê³¼ ì•½ì ì„ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. í•µì‹¬ ìš”ì†Œê°€ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ë¶€ì¡±í•œ ê²½ìš° ë¶„ëª…íˆ ì§€ì í•´ì£¼ì„¸ìš”. **ë‹µë³€ì—ì„œ ë°˜ë§ì´ë‚˜ ì˜ˆì˜ ì—†ëŠ” í‘œí˜„ì„ ì‚¬ìš©í–ˆë‹¤ë©´ ì´ëŠ” ë©´ì ‘ì—ì„œ ì ˆëŒ€ ìš©ë‚©ë  ìˆ˜ ì—†ëŠ” ì¤‘ëŒ€í•œ ë¬¸ì œì ìœ¼ë¡œ ê°•í•˜ê²Œ ì§€ì í•´ì£¼ì„¸ìš”.** í‰ê°€ëŠ” ì „ë¬¸ì ì´ë©´ì„œë„ ì¼ê´€ëœ ì–´ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
3. ğŸ”§ ê°œì„  ë°©ë²•: ë©´ì ‘ìê°€ ë‹µë³€ì„ ì–´ë–»ê²Œ ë³´ì™„í•˜ë©´ ì¢‹ì„ì§€ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”. ê°œì„  ì œì•ˆì€ ê±´ì„¤ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
4. [ìµœì¢… ì ìˆ˜]: 100ì  ë§Œì  ê¸°ì¤€ìœ¼ë¡œ ì •ìˆ˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ì£¼ì„¸ìš”. ì ìˆ˜ë¥¼ í›„í•˜ê²Œ ì£¼ì§€ ë§ê³  ëƒ‰ì •í•˜ê²Œ íŒë‹¨í•´ì£¼ì„¸ìš”. **ë‹µë³€ì—ì„œ ë°˜ë§ì´ë‚˜ ì˜ˆì˜ ì—†ëŠ” í‘œí˜„(~í•´, ~ì•¼, ~ë‹¤ ë“±)ì„ ì‚¬ìš©í–ˆë‹¤ë©´ ì ìˆ˜ë¥¼ ë°˜ìœ¼ë¡œ ê¹ë˜ ì†Œìˆ˜ì ì€ ë²„ë¦¬ê³  ì •ìˆ˜ë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.**
"""

def call_llm(prompt):
    """
    OpenAI GPT-4oë¥¼ í˜¸ì¶œí•˜ì—¬ í‰ê°€ ê²°ê³¼ ìƒì„±
    
    Args:
        prompt (str): GPT-4oì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
        
    Returns:
        str: GPT-4oì˜ ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ì¸ì‚¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤. ì¼ê´€ëœ ì–´ì¡°ì™€ ë§íˆ¬ë¡œ í‰ê°€ì™€ í”¼ë“œë°±ì„ ì œê³µí•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ì¶œë ¥ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2
    )
    return response.choices[0].message.content.strip()

def parse_llm_result(llm_result):
    """
    GPT-4oì˜ ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
    
    Args:
        llm_result (str): GPT-4oì˜ ì›ì‹œ ì‘ë‹µ í…ìŠ¤íŠ¸
        
    Returns:
        tuple: (ìµœì¢…ì ìˆ˜, ì§ˆë¬¸ì˜ë„, í‰ê°€ë‚´ìš©, ê°œì„ ë°©ì•ˆ)
    """
    intent_match = re.search(r"1\. ğŸ“ ì§ˆë¬¸ ì˜ë„:\s*(.+?)\n2\.", llm_result, re.DOTALL)
    eval_match = re.search(r"2\. ğŸ’¬ í‰ê°€:\s*(.+?)\n3\.", llm_result, re.DOTALL)
    improve_match = re.search(r"3\. ğŸ”§ ê°œì„  ë°©ë²•:\s*(.+?)\n4\.", llm_result, re.DOTALL)
    score_match = re.search(r"4\. \[ìµœì¢… ì ìˆ˜\]:\s*(\d+)", llm_result)

    intent = intent_match.group(1).strip() if intent_match else ""
    evaluation = eval_match.group(1).strip() if eval_match else ""
    improvement = improve_match.group(1).strip() if improve_match else ""
    score = int(score_match.group(1)) if score_match else None

    return score, intent, evaluation, improvement



def build_overall_prompt(final_results):
    per_q = ""
    for i, item in enumerate(final_results, 1):
        per_q += f"{i}. ì§ˆë¬¸: {item['question']}\n   ë‹µë³€: {item['answer']}\n   ì ìˆ˜: {item['final_score']}\n"
    return fr"""
[ì „ì²´ ë‹µë³€ í‰ê°€]
ì•„ë˜ëŠ” ì§€ì›ìì˜ ê° ë¬¸í•­ë³„ ë‹µë³€, ì ìˆ˜ì…ë‹ˆë‹¤.

{per_q}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•´ ì§€ì›ìì— ëŒ€í•´
- ìµœì¢… ì ìˆ˜(100ì  ë§Œì , ì •ìˆ˜)
- ì „ì²´ í”¼ë“œë°±(5~10ë¬¸ì¥, êµ¬ì²´ì ì´ê³  ê¸¸ê²Œ, ì „ë¬¸ì ì´ê³  ì¼ê´€ëœ ì–´ì¡°ë¡œ)
- 1ì¤„ ìš”ì•½(í•œ ë¬¸ì¥, ì„íŒ©íŠ¸ ìˆê²Œ)
ë¥¼ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

[ìµœì¢… ì ìˆ˜]: XX
[ì „ì²´ í”¼ë“œë°±]: ...
[1ì¤„ ìš”ì•½]: ...
"""

def parse_overall_llm_result(llm_result):
    score_match = re.search(r"\[ìµœì¢… ì ìˆ˜\]:\s*(\d+)", llm_result)
    feedback_match = re.search(r"\[ì „ì²´ í”¼ë“œë°±\]:\s*(.+?)(?:\n\[|$)", llm_result, re.DOTALL)
    summary_match = re.search(r"\[1ì¤„ ìš”ì•½\]:\s*(.+)", llm_result)
    score = int(score_match.group(1)) if score_match else None
    feedback = feedback_match.group(1).strip() if feedback_match else ""
    summary = summary_match.group(1).strip() if summary_match else ""
    return score, feedback, summary


def process_realtime_results(realtime_data, company_info):
    """
    ì‹¤ì‹œê°„ í‰ê°€ ê²°ê³¼ë¥¼ ìµœì¢… í‰ê°€ í˜•íƒœë¡œ ë³€í™˜
    
    Args:
        realtime_data (list): ì‹¤ì‹œê°„ ê²°ê³¼ ë°ì´í„°
        company_info (dict): DBì—ì„œ ê°€ì ¸ì˜¨ íšŒì‚¬ ì •ë³´
        
    Returns:
        list: ìµœì¢… í‰ê°€ í˜•íƒœë¡œ ë³€í™˜ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    
    final_results = []
    
    # ê° ì§ˆë¬¸ì— ëŒ€í•´ ìµœì¢… í†µí•© í‰ê°€ ìˆ˜í–‰
    for item in realtime_data:
        question = item["question"]
        answer = item["answer"]
        intent = item.get("intent", "")
        ml_score = item.get("ml_score", 0)  # num_eval.pyì—ì„œ ìƒì„±ëœ ì ìˆ˜
        llm_evaluation = item.get("llm_evaluation", "")  # text_eval.pyì—ì„œ ìƒì„±ëœ í‰ê°€
        
        # ML ì ìˆ˜ì™€ LLM í‰ê°€ë¥¼ ê²°í•©í•œ ìµœì¢… í†µí•© í‰ê°€
        final_prompt = build_final_prompt(question, answer, ml_score, llm_evaluation)
        final_result = call_llm(final_prompt)
        
        # ê²°ê³¼ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œ
        final_score, parsed_intent, evaluation, improvement = parse_llm_result(final_result)
        
        # ìµœì¢… ê²°ê³¼ í˜•íƒœë¡œ êµ¬ì„±
        final_results.append({
            "question": question,
            "answer": answer,
            "intent": intent or parsed_intent,
            "final_score": final_score,
            "evaluation": evaluation,
            "improvement": improvement
        })
    
    return final_results

def run_final_evaluation_from_realtime(realtime_data=None, company_info=None, realtime_file="realtime_result.json", output_file="final_evaluation_results.json"):
    """
    ì‹¤ì‹œê°„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… í‰ê°€ ì‹¤í–‰
    
    Args:
        realtime_data (list): ì‹¤ì‹œê°„ ê²°ê³¼ ë°ì´í„° (ìš°ì„ ìˆœìœ„)
        company_info (dict): DBì—ì„œ ê°€ì ¸ì˜¨ íšŒì‚¬ ì •ë³´ (ìš°ì„ ìˆœìœ„)
        realtime_file (str): ì‹¤ì‹œê°„ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ (í´ë°±)
        output_file (str): ìµœì¢… ê²°ê³¼ ì €ì¥ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        dict: ìµœì¢… í‰ê°€ ê²°ê³¼ (ê°œë³„+ì „ì²´)
    """
    print("ìµœì¢… í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")
    
    # 1. ë°ì´í„° ì¤€ë¹„ (ë©”ëª¨ë¦¬ ë°ì´í„° ìš°ì„ , íŒŒì¼ í´ë°±)
    if realtime_data is None:
        # íŒŒì¼ì—ì„œ ì½ê¸° (ê¸°ì¡´ ë°©ì‹)
        with open(realtime_file, "r", encoding="utf-8") as f:
            realtime_data = json.load(f)
    
    if company_info is None:
        # íŒŒì¼ì—ì„œ ì½ê¸° (ê¸°ì¡´ ë°©ì‹)
        with open("company_info.json", "r", encoding="utf-8") as f:
            company_info = json.load(f)
    
    # 2. ì‹¤ì‹œê°„ ê²°ê³¼ë¥¼ ìµœì¢… í‰ê°€ í˜•íƒœë¡œ ë³€í™˜
    #    (MLì ìˆ˜ + LLMí‰ê°€ â†’ í†µí•©ì ìˆ˜ + ìƒì„¸í‰ê°€)
    per_question = process_realtime_results(realtime_data, company_info)
    
    # 3. ì „ì²´ ë©´ì ‘ì— ëŒ€í•œ ì¢…í•© í‰ê°€ ìˆ˜í–‰
    overall_prompt = build_overall_prompt(per_question)
    overall_result = call_llm(overall_prompt)
    overall_score, overall_feedback, summary = parse_overall_llm_result(overall_result)
    
    # 4. ìµœì¢… ê²°ê³¼ êµ¬ì„± (ê¸°ì¡´ í¬ë§·ê³¼ ë™ì¼)
    final_results = {
        "per_question": per_question,      # ê°œë³„ ì§ˆë¬¸ í‰ê°€ ê²°ê³¼
        "overall_score": overall_score,    # ì „ì²´ ì ìˆ˜
        "overall_feedback": overall_feedback,  # ì „ì²´ í”¼ë“œë°±
        "summary": summary                 # 1ì¤„ ìš”ì•½
    }
    
    # 5. JSON íŒŒì¼ë¡œ ì €ì¥ (output_fileì´ ì œê³µëœ ê²½ìš°ì—ë§Œ)
    if output_file:
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        print(f"ê²°ê³¼ëŠ” '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    print(f"ìµœì¢… í‰ê°€ ì™„ë£Œ! ì ìˆ˜: {overall_score}/100")
    
    return final_results

if __name__ == "__main__":
    # ê¸°ì¡´ batch í‰ê°€ ë°©ì‹
    # run_final_eval("interview_data.json")
    
    # ìƒˆë¡œìš´ ì‹¤ì‹œê°„ ê²°ê³¼ ê¸°ë°˜ ìµœì¢… í‰ê°€
    if os.path.exists("realtime_result.json"):
        run_final_evaluation_from_realtime()
    else:
        print("realtime_result.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. process_single_qa.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")